{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n[YouTube URL] → [Extract Transcript]\n                      ↓\n                  [NER Pipeline]\n                      ↓\n               [Extract Facts / Claims]\n                      ↓\n             [Fact-check with Gemini]\n                      ↓\n         [Optionally: Google Search / SerpAPI]\n                      ↓\n       [Return verdict + explanation + URL]\n'''\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"PQKtH7PcPTqk","outputId":"e0b7130c-940c-471d-dce2-6c26595d7c47","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:46:38.449009Z","iopub.execute_input":"2025-05-22T13:46:38.449249Z","iopub.status.idle":"2025-05-22T13:46:38.459835Z","shell.execute_reply.started":"2025-05-22T13:46:38.449227Z","shell.execute_reply":"2025-05-22T13:46:38.458993Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\n[YouTube URL] → [Extract Transcript]\\n                      ↓\\n                  [NER Pipeline]\\n                      ↓\\n               [Extract Facts / Claims]\\n                      ↓\\n             [Fact-check with Gemini]\\n                      ↓\\n         [Optionally: Google Search / SerpAPI]\\n                      ↓\\n       [Return verdict + explanation + URL]\\n'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!pip install google-generativeai\n!pip install youtube-transcript-api\n!pip install langchain\n!pip install langchain-google-genai\n!pip install transformers\n# !pip install serpapi\n!pip install dateparser\n!pip install spacy\n!python -m spacy download en_core_web_sm\n!pip install -U langchain-community\n!pip install duckduckgo-search\n!pip install google-search-results","metadata":{"id":"Tp9Edjs8PiMT","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b0178e63-a391-4d0d-8c1e-cb289f47d65b","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:46:38.560768Z","iopub.execute_input":"2025-05-22T13:46:38.561335Z","iopub.status.idle":"2025-05-22T13:47:41.944361Z","shell.execute_reply.started":"2025-05-22T13:46:38.561304Z","shell.execute_reply":"2025-05-22T13:47:41.943359Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\nRequirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (1.34.1)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.40.1)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (3.20.3)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.72.0rc1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.49.0rc1)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\nCollecting youtube-transcript-api\n  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2025.4.26)\nDownloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: youtube-transcript-api\nSuccessfully installed youtube-transcript-api-1.0.3\nRequirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.49->langchain)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\nDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed packaging-24.2\nCollecting langchain-google-genai\n  Downloading langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\nCollecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\nCollecting langchain-core<0.4.0,>=0.3.52 (from langchain-google-genai)\n  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.4)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.34.1)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.40.1)\nRequirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.20.3)\nRequirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.3.23)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (6.0.2)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.13.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.72.0rc1)\nRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.49.0rc1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.23.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.14.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.126->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.3.1)\nDownloading langchain_google_genai-2.1.4-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: filetype, langchain-core, google-ai-generativelanguage, langchain-google-genai\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.50\n    Uninstalling langchain-core-0.3.50:\n      Successfully uninstalled langchain-core-0.3.50\n  Attempting uninstall: google-ai-generativelanguage\n    Found existing installation: google-ai-generativelanguage 0.6.15\n    Uninstalling google-ai-generativelanguage-0.6.15:\n      Successfully uninstalled google-ai-generativelanguage-0.6.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-core-0.3.60 langchain-google-genai-2.1.4\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nCollecting dateparser\n  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2.9.0.post0)\nRequirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2025.2)\nRequirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2024.11.6)\nRequirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (5.3.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\nDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: dateparser\nSuccessfully installed dateparser-1.2.1\nRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\nRequirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nCollecting langchain-community\n  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.60)\nCollecting langchain<1.0.0,>=0.3.25 (from langchain-community)\n  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.18)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.25->langchain-community)\n  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.13.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\nDownloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, httpx-sse, pydantic-settings, langchain-text-splitters, langchain, langchain-community\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.7\n    Uninstalling langchain-text-splitters-0.3.7:\n      Successfully uninstalled langchain-text-splitters-0.3.7\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.22\n    Uninstalling langchain-0.3.22:\n      Successfully uninstalled langchain-0.3.22\nSuccessfully installed httpx-sse-0.4.0 langchain-0.3.25 langchain-community-0.3.24 langchain-text-splitters-0.3.8 pydantic-settings-2.9.1 python-dotenv-1.1.0\nCollecting duckduckgo-search\n  Downloading duckduckgo_search-8.0.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\nCollecting primp>=0.15.0 (from duckduckgo-search)\n  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.1)\nDownloading duckduckgo_search-8.0.2-py3-none-any.whl (18 kB)\nDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: primp, duckduckgo-search\nSuccessfully installed duckduckgo-search-8.0.2 primp-0.15.0\nCollecting google-search-results\n  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from google-search-results) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->google-search-results) (2025.4.26)\nBuilding wheels for collected packages: google-search-results\n  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=487cf53ae7dda4edf9449757a7a172b11b7ae2ec8538d4cebd3753434d1963a7\n  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\nSuccessfully built google-search-results\nInstalling collected packages: google-search-results\nSuccessfully installed google-search-results-2.4.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport dateparser\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom transformers import pipeline\nfrom langchain.tools import Tool\nfrom langchain.agents import initialize_agent, AgentType\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.utilities import SerpAPIWrapper\nfrom langchain.chat_models import ChatOpenAI\nimport spacy\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom langchain.tools import Tool\nimport time","metadata":{"id":"0mzlqOTDPm88","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:47:41.946507Z","iopub.execute_input":"2025-05-22T13:47:41.947157Z","iopub.status.idle":"2025-05-22T13:48:14.396044Z","shell.execute_reply.started":"2025-05-22T13:47:41.947117Z","shell.execute_reply":"2025-05-22T13:48:14.395353Z"}},"outputs":[{"name":"stderr","text":"2025-05-22 13:47:51.470118: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747921671.718585      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747921671.788508      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"os.environ[\"GOOGLE_API_KEY\"] = \"\"\nvideo_url = \"https://www.youtube.com/watch?v=JIbIYCM48to&t=99s\"","metadata":{"id":"RZ9fd5anPqnR","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:48:14.396844Z","iopub.execute_input":"2025-05-22T13:48:14.397559Z","iopub.status.idle":"2025-05-22T13:48:14.401523Z","shell.execute_reply.started":"2025-05-22T13:48:14.397536Z","shell.execute_reply":"2025-05-22T13:48:14.400753Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_youtube_transcript(video_url):\n\n    video_id = video_url.split(\"v=\")[-1]\n    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n    text = \" \".join([item['text'] for item in transcript])\n\n    return text\ntext = get_youtube_transcript(video_url)\nprint(text)","metadata":{"id":"0uxcHOnGPtlk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"62efce00-c16a-458a-b74d-f23a0b66bf9d","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:48:14.402445Z","iopub.execute_input":"2025-05-22T13:48:14.402731Z","iopub.status.idle":"2025-05-22T13:48:15.327148Z","shell.execute_reply.started":"2025-05-22T13:48:14.402712Z","shell.execute_reply":"2025-05-22T13:48:15.326329Z"}},"outputs":[{"name":"stdout","text":"Amazon web services launched in 2006 with a total of three products storage buckets compute instances and a messaging queue today it offers a mind-numbing 200 and something services and what's most confusing is that many of them appear to do almost the exact same thing it's kind of like shopping at a big grocery store where you have different aisles of product categories filled with things to buy that meet the needs of virtually every developer on the planet in today's video we'll walk down these aisles to gain an understanding of over 50 different AWS products so first let's start with a few that are above my pay grade that you may not know exist if you're building robots you can use robomaker to simulate and test your robots at scale then once your robots are in people's homes you can use iot core to collect data from them update their software and manage them remotely if you happen to have a satellite orbiting Earth you can tap into Amazon's Global Network of antennas to connect data through its ground station service and if you want to start experimenting and researching the future of computing you can use bracket to interact with a quantum computer but most developers go to the cloud to solve more practical problems and for that let's head to the compute aisle one of the original AWS products was elastic compute Cloud it's one of the most fundamental building blocks on the platform and allows you to create a virtual computer in the cloud choose your operating system memory and computing power then you can rent that space in the cloud like you're renting an apartment that you pay for by the second a common use case is to use an instance as a server for web application but one problem is that as your app grows you'll likely need to distribute track traffic across multiple instances in 2009 Amazon introduced elastic load balancing which allowed developers to distribute traffic to multiple instances automatically in addition the cloudwatch service can collect logs and metrics from each individual instance the data collected from cloudwatch can then be passed off to autoscale in which you define policies that create new instances as they become needed based on the traffic and utilization of your current infrastructure these tools were revolutionary at the time but developers still wanted an easier way to get things done and that's where elastic beanock comes in most developers in 2011 just wanted to deploy a Ruby on Rails app elastic beanock made that much easier by providing an additional layer of abstraction on top of ec2 and other autoscaling features choose a template deploy your code and let all the autoscaling stuff happen automatically this is often called a platform as a service but in some cases it's still too complicated if you don't care about the underlying infrastructure whatsoever and just want to deploy a WordPress site light sale is alternative option where you can point and click at what you want to deploy and worry even less about the underlying configuration in all these cases you are deploying a static server that is always running in the cloud but many Computing jobs are ephemeral which means they don't rely on any persistent State on the server so why bother deploying a server for code like that in 2014 Lambda came out which are functions as a service or serverless Computing with Lambda you simply upload your code then choose an event that decides when that code should run traffic scaling and netor networking are all things that happen entirely in the background and unlike a dedicated server you only pay for the exact number of request and Computing time that you use now if you don't like writing your own code you can use the serverless application repository to find pre-built functions that you can deploy with the click of a button but what if you're a huge Enterprise with a bunch of its own servers Outpost is a way to rent AWS apis on your own infrastructure without needing to throw your old servers in the garbage in other cases you may want to interact with AWS from remote or extreme environment like if you're a scientist in the Arctic snow devices are like little mini data centers that can work without internet in hostile environments so that gives us some fundamental ways to compute things but many apps today are standardized with Docker containers allowing them to run on multiple different clouds or Computing environments with very little effort to run a container you first need to create a Docker image and store it somewhere elastic container registry allows you to upload an image allowing other tools like elastic container service to pull it back down and run it ECS is an API for starting stopping and allocating virtual machines to your containers and allows you to connect them to other products like load balancers some companies may want more control over how their app scales in which case eks is a tool for running kubernetes but in other cases you may want your containers to behave in a more automated way fargate is a tool that will make your containers behave like serverless functions removing the need to allocate ec2 instances for your containers but if you're building an application and already have it containerized the easiest way to deploy it to AWS is app Runner this is a new product in 2021 where you simply point it to a container image while it handles all the orchestration and scaling behind the scenes but running an application is only half the battle we also need to store data in the cloud simple storage service or S3 was the very first product offered by AWS it can store any type of file or object like an image or video and it's based on the same infrastructure as Amazon's e-commerce site it's great for general purpose file storage but if you don't access your files very often you can archive them in Glacier which has a higher latency but a much lower cost on the other end of the spectrum you may need storage that is extremely fast and can handle a lot of throughput elastic block storage is ideal for applications that have intensive data processing requirements but requires more manual configuration by the developer now if you want something that's highly performant and also fully managed elastic file system provides all the bells and whistles but at a much higher cost in addition to raw files developers also need to store structured data for their end users and that brings us to the database aisle which has a lot of different products to choose from the first ever database on AWS was simple DB a general purpose nosql database but it tends to be a little too simple for most people everybody knows you never go full R time it was followed up a few years later with Dynamo DB which is a document database that's very easy to scale horizontally it's inexpensive and provides fast read performance but it isn't very good at modeling relational data if you're familiar with mongodb another document database option is document DB it's a controversial option that's technically not mongodb that has a one toone mapping of the mongod DB API to get around restrictive open source licensing speaking of which Amazon also did a similar thing with elastic search which itself is a great option if you want to build something like a fulltech search engine but the majority of developers out there will opt for a traditional relational SQL database Amazon relational database service RDS supports a variety of different SQL flavors and can fully manage things like backups patching and scale but Amazon also offers its own proprietary flavor of SQL called Aurora it's compatible with postgress or MySQL and can be operated with better performance at a lower cost in addition Aurora offers a new serverless option that makes it even easier to scale and you only pay for the actual time that the database is in use relational databases are a great general purpose option but they're not the only option Neptune is a graph database that can achieve better performance on highly connected data sets like a social graph or recommendation engine if your current database is too slow you may want to bring in elastic cache which is a fully managed version of redis in inmemory database that delivers data to your end users with extremely low latency if you work with time series data like the stock market for example you might benefit from time stream a Time series database with built-in functions for time based queries and additional features for analytics yet another option is the Quantum Ledger database which allows you to build an immutable set of cryptographically signed transactions very similar to decentralized blockchain technology now let's shift gears and talk about analytics to analyze data you first need a place to store it and a popular option for doing that is red shift which is a data warehouse that tries to get you to shift away from Oracle warehouses are often used by big Enterprises to dump multiple data sources from the business where they can be analyzed together when all your data is in one place it's easier to generate meing meanful analytics and run machine learning on it data in a warehouse is structured so it can be queried but if you need a place to put a large amount of unstructured data you can use AWS Lake formation which is a tool for creating data lakes or repositories that store a large amount of unstructured data which can be used in addition to data warehouses to query a larger variety of data sources if you want to analyze real-time data you can use Kinesis to capture real-time streams from your infrastructure then visualize them in your favorite business and elligence tool or you can use a stream processing framework like Apache spark that runs on elastic map reduce which itself is a service that allows you to operate on massive data sets efficiently with a parallel distributed algorithm now if you don't want to use Kinesis for streaming data a popular alternative is Apache kofka it's open source and Amazon msk is a fully managed service to get you started but for the average developer all this data processing may be a little too complicated glue is a serverless product that makes it much easier to extract transform and load your data it can automatically connect to other data sources on AWS like Aurora red shift and S3 and has a tool called glue studio so you can create jobs without having to write any actual source code but one of the biggest advantages of collecting massive amounts of data is that you can use it to help predict the future and AWS has a bunch of tools in the machine learning aisle to make that process easier but first if you don't have any highquality data of your own you can use the data exchange to purchase And subscribe rbe to data from third party sources once you have some data in the cloud you can use sag maker to connect to it and start building machine learning models with tensor flow or P torch it operates on multiple levels to make machine learning easier and provides a managed Jupiter notebook that can connect to a GPU instance to train a machine learning model then deploy it somewhere useful that's cool but building your own ml models from scratch is still extremely difficult if you need to do image analysis you may as well just use the recognition API it can classify all kinds of objects and images and is likely way better than anything that you would build on your own or if you want to build a conversational bot you might use Lex which runs on the same technology that powers Alexa devices or if you just want to have fun and learn how machine Learning Works you might buy a deep raser device which is an actual Ras car that you can drive with your own machine learning code now that's a pretty amazing way to get people to use your Cloud platform but let's change direction and look at a few other essential tools that are used by a wide variety of Developers for security we have IM where you can can create rules and determine who has access to what on your AWS account if you're building a web or mobile app where users can log into an account Cognito is a tool that enables them to log in with a variety of different authentication methods and manages the user sessions for you then once you have a few users logged into your app you may want to send them push notifications SNS is a tool that can get that job done or maybe you want to send emails to your users SES is the tool for that now that you know about all these tools you're going to want an organized way to provision them cloud formation is a way to create templates based on your infrastructure in yaml or Json allowing you to enable hundreds of different services with the single click of a button from there you'll likely want to interact with those services from a front-end application like iOS Android or the web amplify provides sdks that can connect to your infrastructure from JavaScript Frameworks and other front-end applications now the final thing to remember is that all of this is going to cost you a ton of money which goes directly to getting Jeff's rocket up so make sure to use AWS cost Explorer and budgets if you don't want to pay for these big bul jeene Rockets that's the end of the video it took a ton of work so please like And subscribe to support the channel or become a prom member at fireship iio to get access to more advanced content about building apps in the cloud thanks for watching and I will see you in the next one\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n\n# Load spaCy and Hugging Face NER model\nnlp = spacy.load(\"en_core_web_sm\")\n\nfrom transformers import pipeline\nimport spacy\nimport dateparser\n\n# Load models\nner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef extract_factual_claims(text, window_size=1):\n    doc = nlp(text)\n    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) >= 5]\n    extracted_claims = []\n\n    for idx, sentence_text in enumerate(sentences):\n        entities = ner_pipeline(sentence_text)\n\n        if entities:\n            # Get surrounding context sentences within the window size\n            start_idx = max(0, idx - window_size)\n            end_idx = min(len(sentences), idx + window_size + 1)\n            context_sentences = sentences[start_idx:end_idx]\n            context_text = \" \".join(context_sentences)\n\n            claim_info = {\n                \"sentence\": sentence_text,\n                \"context\": context_text,\n                \"entities\": []\n            }\n            for ent in entities:\n                entity_data = {\n                    \"text\": ent['word'],\n                    \"type\": ent['entity_group'],\n                    \"score\": ent['score']\n                }\n                if ent['entity_group'] == \"DATE\":\n                    parsed_date = dateparser.parse(ent['word'])\n                    entity_data[\"parsed_date\"] = str(parsed_date) if parsed_date else None\n                claim_info[\"entities\"].append(entity_data)\n\n            extracted_claims.append(claim_info)\n\n    return extracted_claims\n\n\n# Example\nclaims = extract_factual_claims(text)\nfor c in claims[:5]:\n    print(c)\n","metadata":{"id":"SVQ5lRzMPupi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad9aab18-2c98-49cc-cfe2-da12c3bd4513","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T13:48:15.328957Z","iopub.execute_input":"2025-05-22T13:48:15.329260Z","iopub.status.idle":"2025-05-22T13:48:40.382342Z","shell.execute_reply.started":"2025-05-22T13:48:15.329239Z","shell.execute_reply":"2025-05-22T13:48:40.381521Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e337868eb1354e71abfa33347dd586b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39fed384227a4d9dbd00be28748800d4"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11d991de1a784efd86eb5033ba518416"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a8677397a54476916262d7cee9f7d3"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\nSome weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"{'sentence': \"Amazon web services launched in 2006 with a total of three products storage buckets compute instances and a messaging queue today it offers a mind-numbing 200 and something services and what's most confusing is that many of them appear to do almost the exact same thing it's kind of like shopping at a big grocery store where you have different aisles of product categories filled with things to buy that meet the needs of virtually every developer on the planet in today's video we'll walk down these aisles to gain an understanding of over 50 different AWS products so first let's start with a few that are above my pay grade that you may not know exist if you're building robots you can use robomaker to simulate and test your robots at scale then once your robots are in people's homes you can use iot core to collect data from them update their software and manage them remotely if you happen to have a satellite orbiting Earth you can tap into Amazon's Global Network of antennas to connect data through its ground station service and if you want to start experimenting and researching the future of computing you can use bracket to interact with a quantum computer but most developers go to the cloud to solve more practical problems and for that let's head to the compute aisle one of the original AWS products was elastic compute\", 'context': \"Amazon web services launched in 2006 with a total of three products storage buckets compute instances and a messaging queue today it offers a mind-numbing 200 and something services and what's most confusing is that many of them appear to do almost the exact same thing it's kind of like shopping at a big grocery store where you have different aisles of product categories filled with things to buy that meet the needs of virtually every developer on the planet in today's video we'll walk down these aisles to gain an understanding of over 50 different AWS products so first let's start with a few that are above my pay grade that you may not know exist if you're building robots you can use robomaker to simulate and test your robots at scale then once your robots are in people's homes you can use iot core to collect data from them update their software and manage them remotely if you happen to have a satellite orbiting Earth you can tap into Amazon's Global Network of antennas to connect data through its ground station service and if you want to start experimenting and researching the future of computing you can use bracket to interact with a quantum computer but most developers go to the cloud to solve more practical problems and for that let's head to the compute aisle one of the original AWS products was elastic compute Cloud\", 'entities': [{'text': 'Amazon', 'type': 'ORG', 'score': 0.9959694}, {'text': 'AWS', 'type': 'ORG', 'score': 0.93882024}, {'text': 'Earth', 'type': 'LOC', 'score': 0.8334735}, {'text': 'Amazon', 'type': 'ORG', 'score': 0.9943884}, {'text': 'AWS', 'type': 'ORG', 'score': 0.97679555}]}\n{'sentence': 'Cloud', 'context': \"Amazon web services launched in 2006 with a total of three products storage buckets compute instances and a messaging queue today it offers a mind-numbing 200 and something services and what's most confusing is that many of them appear to do almost the exact same thing it's kind of like shopping at a big grocery store where you have different aisles of product categories filled with things to buy that meet the needs of virtually every developer on the planet in today's video we'll walk down these aisles to gain an understanding of over 50 different AWS products so first let's start with a few that are above my pay grade that you may not know exist if you're building robots you can use robomaker to simulate and test your robots at scale then once your robots are in people's homes you can use iot core to collect data from them update their software and manage them remotely if you happen to have a satellite orbiting Earth you can tap into Amazon's Global Network of antennas to connect data through its ground station service and if you want to start experimenting and researching the future of computing you can use bracket to interact with a quantum computer but most developers go to the cloud to solve more practical problems and for that let's head to the compute aisle one of the original AWS products was elastic compute Cloud it's one of the most fundamental building blocks on the platform and allows you to create a virtual computer in the cloud choose your operating system memory and computing power then you can rent that space in the cloud like you're renting an apartment that you pay for by the second a common use case is to use an instance as a server for web application but one problem is that as your app grows you'll likely need to distribute track traffic across multiple instances in 2009 Amazon introduced elastic load balancing which allowed developers to distribute traffic to multiple instances automatically in addition the cloudwatch service can collect logs and metrics from each individual instance the data collected from cloudwatch can then be passed off to autoscale in which you define policies that create new instances as they become needed based on the traffic and utilization of your current infrastructure these tools were revolutionary at the time but developers still wanted an easier way to get things done and that's where elastic beanock comes in most developers in 2011 just wanted to deploy a Ruby on Rails app elastic beanock made that much easier by providing an additional layer of abstraction on top of ec2 and other autoscaling features choose a template deploy your code and let all the autoscaling stuff happen automatically this is often called a platform as a service but in some cases it's still too complicated if you don't care about the underlying infrastructure whatsoever and just want to deploy a WordPress site light sale is alternative option where you can point and click at what you want to deploy and worry even less about the underlying configuration in all these cases you are deploying a static server that is always running in the cloud but many Computing jobs are ephemeral which means they don't rely on any persistent State on the server so why bother deploying a server for code like that in 2014 Lambda came out which are functions as a service or serverless Computing with Lambda you simply upload your code then choose an event that decides when that code should run traffic scaling and netor networking are all things that happen entirely in the background and unlike a dedicated server you only pay for the exact number of request and Computing time that you use now if you don't like writing your own code you can use the serverless application repository to find pre-built functions that you can deploy with the click of a button but what if you're a huge Enterprise with a bunch of its own servers Outpost is a way to rent AWS apis on your own infrastructure without needing to throw your old servers in the garbage in other cases you may want to interact with AWS from remote or extreme environment like if you're a scientist in the Arctic snow devices are like little mini data centers that can work without internet in hostile environments so that gives us some fundamental ways to compute things but many apps today are standardized with Docker containers allowing them to run on multiple different clouds or Computing environments with very little effort to run a container you first need to create a Docker image and store it somewhere elastic container registry allows you to upload an image allowing other tools like elastic container service to pull it back down and run it ECS is an API for starting stopping and allocating virtual machines to your containers and allows you to connect them to other products like load balancers some companies may want more control over how their app scales in which case eks is a tool for running kubernetes but in other cases you may want your containers to behave in a more automated way fargate is a tool that will make your containers behave like serverless functions removing the need to allocate ec2 instances for your containers but if you're building an application and already have it containerized the easiest way to deploy it to AWS is app Runner this is a new product in 2021 where you simply point it to a container image while it handles all the orchestration and scaling behind the scenes but running an application is only half the battle we also need to store data in the cloud simple storage service or S3 was the very first product offered by AWS it can store any type of file or object like an image or video and it's based on the same infrastructure as Amazon's e-commerce site it's great for general purpose file storage but if you don't access your files very often you can archive them in Glacier which has a higher latency but a much lower cost on the other end of the spectrum you may need storage that is extremely fast and can handle a lot of throughput elastic block storage is ideal for applications that have intensive data processing requirements but requires more manual configuration by the developer now if you want something that's highly performant and also fully managed elastic file system provides all the bells and whistles but at a much higher cost in addition to raw files developers also need to store structured data for their end users and that brings us to the database aisle which has a lot of different products to choose from the first ever database on AWS was simple DB a general purpose nosql database\", 'entities': [{'text': 'Cloud', 'type': 'PER', 'score': 0.8012886}]}\n{'sentence': \"it's one of the most fundamental building blocks on the platform and allows you to create a virtual computer in the cloud choose your operating system memory and computing power then you can rent that space in the cloud like you're renting an apartment that you pay for by the second a common use case is to use an instance as a server for web application but one problem is that as your app grows you'll likely need to distribute track traffic across multiple instances in 2009 Amazon introduced elastic load balancing which allowed developers to distribute traffic to multiple instances automatically in addition the cloudwatch service can collect logs and metrics from each individual instance the data collected from cloudwatch can then be passed off to autoscale in which you define policies that create new instances as they become needed based on the traffic and utilization of your current infrastructure these tools were revolutionary at the time but developers still wanted an easier way to get things done and that's where elastic beanock comes in most developers in 2011 just wanted to deploy a Ruby on Rails app elastic beanock made that much easier by providing an additional layer of abstraction on top of ec2 and other autoscaling features choose a template deploy your code and let all the autoscaling stuff happen automatically this is often called a platform as a service but in some cases it's still too complicated if you don't care about the underlying infrastructure whatsoever and just want to deploy a WordPress site light sale is alternative option where you can point and click at what you want to deploy and worry even less about the underlying configuration in all these cases you are deploying a static server that is always running in the cloud but many Computing jobs are ephemeral which means they don't rely on any persistent State on the server so why bother deploying a server for code like that in 2014 Lambda came out which are functions as a service or serverless Computing with Lambda you simply upload your code then choose an event that decides when that code should run traffic scaling and netor networking are all things that happen entirely in the background and unlike a dedicated server you only pay for the exact number of request and Computing time that you use now if you don't like writing your own code you can use the serverless application repository to find pre-built functions that you can deploy with the click of a button but what if you're a huge Enterprise with a bunch of its own servers Outpost is a way to rent AWS apis on your own infrastructure without needing to throw your old servers in the garbage in other cases you may want to interact with AWS from remote or extreme environment like if you're a scientist in the Arctic snow devices are like little mini data centers that can work without internet in hostile environments so that gives us some fundamental ways to compute things but many apps today are standardized with Docker containers allowing them to run on multiple different clouds or Computing environments with very little effort to run a container you first need to create a Docker image and store it somewhere elastic container registry allows you to upload an image allowing other tools like elastic container service to pull it back down and run it ECS is an API for starting stopping and allocating virtual machines to your containers and allows you to connect them to other products like load balancers some companies may want more control over how their app scales in which case eks is a tool for running kubernetes but in other cases you may want your containers to behave in a more automated way fargate is a tool that will make your containers behave like serverless functions removing the need to allocate ec2 instances for your containers but if you're building an application and already have it containerized the easiest way to deploy it to AWS is app Runner this is a new product in 2021 where you simply point it to a container image while it handles all the orchestration and scaling behind the scenes but running an application is only half the battle we also need to store data in the cloud simple storage service or S3 was the very first product offered by AWS it can store any type of file or object like an image or video and it's based on the same infrastructure as Amazon's e-commerce site it's great for general purpose file storage but if you don't access your files very often you can archive them in Glacier which has a higher latency but a much lower cost on the other end of the spectrum you may need storage that is extremely fast and can handle a lot of throughput elastic block storage is ideal for applications that have intensive data processing requirements but requires more manual configuration by the developer now if you want something that's highly performant and also fully managed elastic file system provides all the bells and whistles but at a much higher cost in addition to raw files developers also need to store structured data for their end users and that brings us to the database aisle which has a lot of different products to choose from the first ever database on AWS was simple DB a general purpose nosql database\", 'context': \"Cloud it's one of the most fundamental building blocks on the platform and allows you to create a virtual computer in the cloud choose your operating system memory and computing power then you can rent that space in the cloud like you're renting an apartment that you pay for by the second a common use case is to use an instance as a server for web application but one problem is that as your app grows you'll likely need to distribute track traffic across multiple instances in 2009 Amazon introduced elastic load balancing which allowed developers to distribute traffic to multiple instances automatically in addition the cloudwatch service can collect logs and metrics from each individual instance the data collected from cloudwatch can then be passed off to autoscale in which you define policies that create new instances as they become needed based on the traffic and utilization of your current infrastructure these tools were revolutionary at the time but developers still wanted an easier way to get things done and that's where elastic beanock comes in most developers in 2011 just wanted to deploy a Ruby on Rails app elastic beanock made that much easier by providing an additional layer of abstraction on top of ec2 and other autoscaling features choose a template deploy your code and let all the autoscaling stuff happen automatically this is often called a platform as a service but in some cases it's still too complicated if you don't care about the underlying infrastructure whatsoever and just want to deploy a WordPress site light sale is alternative option where you can point and click at what you want to deploy and worry even less about the underlying configuration in all these cases you are deploying a static server that is always running in the cloud but many Computing jobs are ephemeral which means they don't rely on any persistent State on the server so why bother deploying a server for code like that in 2014 Lambda came out which are functions as a service or serverless Computing with Lambda you simply upload your code then choose an event that decides when that code should run traffic scaling and netor networking are all things that happen entirely in the background and unlike a dedicated server you only pay for the exact number of request and Computing time that you use now if you don't like writing your own code you can use the serverless application repository to find pre-built functions that you can deploy with the click of a button but what if you're a huge Enterprise with a bunch of its own servers Outpost is a way to rent AWS apis on your own infrastructure without needing to throw your old servers in the garbage in other cases you may want to interact with AWS from remote or extreme environment like if you're a scientist in the Arctic snow devices are like little mini data centers that can work without internet in hostile environments so that gives us some fundamental ways to compute things but many apps today are standardized with Docker containers allowing them to run on multiple different clouds or Computing environments with very little effort to run a container you first need to create a Docker image and store it somewhere elastic container registry allows you to upload an image allowing other tools like elastic container service to pull it back down and run it ECS is an API for starting stopping and allocating virtual machines to your containers and allows you to connect them to other products like load balancers some companies may want more control over how their app scales in which case eks is a tool for running kubernetes but in other cases you may want your containers to behave in a more automated way fargate is a tool that will make your containers behave like serverless functions removing the need to allocate ec2 instances for your containers but if you're building an application and already have it containerized the easiest way to deploy it to AWS is app Runner this is a new product in 2021 where you simply point it to a container image while it handles all the orchestration and scaling behind the scenes but running an application is only half the battle we also need to store data in the cloud simple storage service or S3 was the very first product offered by AWS it can store any type of file or object like an image or video and it's based on the same infrastructure as Amazon's e-commerce site it's great for general purpose file storage but if you don't access your files very often you can archive them in Glacier which has a higher latency but a much lower cost on the other end of the spectrum you may need storage that is extremely fast and can handle a lot of throughput elastic block storage is ideal for applications that have intensive data processing requirements but requires more manual configuration by the developer now if you want something that's highly performant and also fully managed elastic file system provides all the bells and whistles but at a much higher cost in addition to raw files developers also need to store structured data for their end users and that brings us to the database aisle which has a lot of different products to choose from the first ever database on AWS was simple DB a general purpose nosql database but it tends to be a little too simple for most people everybody knows you never go full R time it was followed up a few years later with Dynamo DB which is a document database that's very easy to scale horizontally it's inexpensive and provides fast read performance\", 'entities': [{'text': 'Amazon', 'type': 'ORG', 'score': 0.9969465}, {'text': 'Ruby on Rails', 'type': 'MISC', 'score': 0.99144137}, {'text': 'WordP', 'type': 'MISC', 'score': 0.84737843}, {'text': '##ress', 'type': 'ORG', 'score': 0.5618992}, {'text': 'Lambda', 'type': 'ORG', 'score': 0.92489445}]}\n{'sentence': \"but it tends to be a little too simple for most people everybody knows you never go full R time it was followed up a few years later with Dynamo DB which is a document database that's very easy to scale horizontally it's inexpensive and provides fast read performance\", 'context': \"it's one of the most fundamental building blocks on the platform and allows you to create a virtual computer in the cloud choose your operating system memory and computing power then you can rent that space in the cloud like you're renting an apartment that you pay for by the second a common use case is to use an instance as a server for web application but one problem is that as your app grows you'll likely need to distribute track traffic across multiple instances in 2009 Amazon introduced elastic load balancing which allowed developers to distribute traffic to multiple instances automatically in addition the cloudwatch service can collect logs and metrics from each individual instance the data collected from cloudwatch can then be passed off to autoscale in which you define policies that create new instances as they become needed based on the traffic and utilization of your current infrastructure these tools were revolutionary at the time but developers still wanted an easier way to get things done and that's where elastic beanock comes in most developers in 2011 just wanted to deploy a Ruby on Rails app elastic beanock made that much easier by providing an additional layer of abstraction on top of ec2 and other autoscaling features choose a template deploy your code and let all the autoscaling stuff happen automatically this is often called a platform as a service but in some cases it's still too complicated if you don't care about the underlying infrastructure whatsoever and just want to deploy a WordPress site light sale is alternative option where you can point and click at what you want to deploy and worry even less about the underlying configuration in all these cases you are deploying a static server that is always running in the cloud but many Computing jobs are ephemeral which means they don't rely on any persistent State on the server so why bother deploying a server for code like that in 2014 Lambda came out which are functions as a service or serverless Computing with Lambda you simply upload your code then choose an event that decides when that code should run traffic scaling and netor networking are all things that happen entirely in the background and unlike a dedicated server you only pay for the exact number of request and Computing time that you use now if you don't like writing your own code you can use the serverless application repository to find pre-built functions that you can deploy with the click of a button but what if you're a huge Enterprise with a bunch of its own servers Outpost is a way to rent AWS apis on your own infrastructure without needing to throw your old servers in the garbage in other cases you may want to interact with AWS from remote or extreme environment like if you're a scientist in the Arctic snow devices are like little mini data centers that can work without internet in hostile environments so that gives us some fundamental ways to compute things but many apps today are standardized with Docker containers allowing them to run on multiple different clouds or Computing environments with very little effort to run a container you first need to create a Docker image and store it somewhere elastic container registry allows you to upload an image allowing other tools like elastic container service to pull it back down and run it ECS is an API for starting stopping and allocating virtual machines to your containers and allows you to connect them to other products like load balancers some companies may want more control over how their app scales in which case eks is a tool for running kubernetes but in other cases you may want your containers to behave in a more automated way fargate is a tool that will make your containers behave like serverless functions removing the need to allocate ec2 instances for your containers but if you're building an application and already have it containerized the easiest way to deploy it to AWS is app Runner this is a new product in 2021 where you simply point it to a container image while it handles all the orchestration and scaling behind the scenes but running an application is only half the battle we also need to store data in the cloud simple storage service or S3 was the very first product offered by AWS it can store any type of file or object like an image or video and it's based on the same infrastructure as Amazon's e-commerce site it's great for general purpose file storage but if you don't access your files very often you can archive them in Glacier which has a higher latency but a much lower cost on the other end of the spectrum you may need storage that is extremely fast and can handle a lot of throughput elastic block storage is ideal for applications that have intensive data processing requirements but requires more manual configuration by the developer now if you want something that's highly performant and also fully managed elastic file system provides all the bells and whistles but at a much higher cost in addition to raw files developers also need to store structured data for their end users and that brings us to the database aisle which has a lot of different products to choose from the first ever database on AWS was simple DB a general purpose nosql database but it tends to be a little too simple for most people everybody knows you never go full R time it was followed up a few years later with Dynamo DB which is a document database that's very easy to scale horizontally it's inexpensive and provides fast read performance but it isn't very good at modeling relational data if you're familiar with mongodb another document database option is\", 'entities': [{'text': 'Dynamo DB', 'type': 'ORG', 'score': 0.5753714}]}\n{'sentence': \"it's a controversial option that's technically not mongodb that has a one toone mapping of the mongod DB API to get around restrictive open source licensing speaking of which Amazon also did a similar thing with elastic search which itself is a great option if you want to build something like a fulltech search engine but the majority of developers out there will opt for a traditional relational SQL database Amazon relational database service RDS supports a variety of different SQL flavors and can fully manage things like backups patching and scale but Amazon also offers its own proprietary flavor of SQL called Aurora it's compatible with postgress or MySQL and can be operated with better performance at a lower cost in addition Aurora offers a new serverless option that makes it even easier to scale and you only pay for the actual time that the database is in use relational databases are a great general purpose option but they're not the only option Neptune is a graph database that can achieve better performance on highly connected data sets like a social graph or recommendation engine if your current database is too slow you may want to bring in elastic cache which is a fully managed version of redis in inmemory database that delivers data to your end users with extremely low latency if you work with time series data like the stock market for example you might benefit from time stream a Time series database with built-in functions for time based queries and additional features for analytics yet another option is the Quantum Ledger database which allows you to build an immutable set of cryptographically signed transactions very similar to decentralized blockchain technology now let's shift gears and talk about analytics to analyze data you first need a place to store it and a popular option for doing that is red shift which is a data warehouse that tries to get you to shift away from Oracle warehouses are often used by big Enterprises to dump multiple data sources from the business where they can be analyzed together when all your data is in one place it's easier to generate meing meanful analytics and run machine learning on it data in a warehouse is structured so it can be queried but if you need a place to put a large amount of unstructured data you can use AWS Lake formation which is a tool for creating data lakes or repositories that store a large amount of unstructured data which can be used in addition to data warehouses to query a larger variety of data sources if you want to analyze real-time data you can use Kinesis to capture real-time streams from your infrastructure then visualize them in your favorite business and elligence tool or you can use a stream processing framework like Apache spark that runs on elastic map reduce which itself is a service that allows you to operate on massive data sets efficiently with a parallel distributed algorithm now if you don't want to use Kinesis for streaming data a popular alternative is Apache kofka\", 'context': \"document DB it's a controversial option that's technically not mongodb that has a one toone mapping of the mongod DB API to get around restrictive open source licensing speaking of which Amazon also did a similar thing with elastic search which itself is a great option if you want to build something like a fulltech search engine but the majority of developers out there will opt for a traditional relational SQL database Amazon relational database service RDS supports a variety of different SQL flavors and can fully manage things like backups patching and scale but Amazon also offers its own proprietary flavor of SQL called Aurora it's compatible with postgress or MySQL and can be operated with better performance at a lower cost in addition Aurora offers a new serverless option that makes it even easier to scale and you only pay for the actual time that the database is in use relational databases are a great general purpose option but they're not the only option Neptune is a graph database that can achieve better performance on highly connected data sets like a social graph or recommendation engine if your current database is too slow you may want to bring in elastic cache which is a fully managed version of redis in inmemory database that delivers data to your end users with extremely low latency if you work with time series data like the stock market for example you might benefit from time stream a Time series database with built-in functions for time based queries and additional features for analytics yet another option is the Quantum Ledger database which allows you to build an immutable set of cryptographically signed transactions very similar to decentralized blockchain technology now let's shift gears and talk about analytics to analyze data you first need a place to store it and a popular option for doing that is red shift which is a data warehouse that tries to get you to shift away from Oracle warehouses are often used by big Enterprises to dump multiple data sources from the business where they can be analyzed together when all your data is in one place it's easier to generate meing meanful analytics and run machine learning on it data in a warehouse is structured so it can be queried but if you need a place to put a large amount of unstructured data you can use AWS Lake formation which is a tool for creating data lakes or repositories that store a large amount of unstructured data which can be used in addition to data warehouses to query a larger variety of data sources if you want to analyze real-time data you can use Kinesis to capture real-time streams from your infrastructure then visualize them in your favorite business and elligence tool or you can use a stream processing framework like Apache spark that runs on elastic map reduce which itself is a service that allows you to operate on massive data sets efficiently with a parallel distributed algorithm now if you don't want to use Kinesis for streaming data a popular alternative is Apache kofka it's open source and Amazon msk is a fully managed service to get you started but for the average developer all this data processing may be a little too complicated glue is a serverless product that makes it much easier to extract transform and load your data it can automatically connect to other data sources on AWS like Aurora red shift and S3 and has a tool called glue studio so you can create jobs without having to write any actual source code but one of the biggest advantages of collecting massive amounts of data is that you can use it to help predict the future and AWS has a bunch of tools in the machine learning aisle to make that process easier but first if you don't have any highquality data of your own you can use the data exchange to purchase And subscribe rbe to data from third party sources once you have some data in the cloud you can use sag maker to connect to it and start building machine learning models with tensor flow or P torch it operates on multiple levels to make machine learning easier and provides a managed Jupiter notebook that can connect to a GPU instance to train a machine learning model then deploy it\", 'entities': [{'text': 'DB API', 'type': 'MISC', 'score': 0.7120721}, {'text': 'Amazon', 'type': 'ORG', 'score': 0.99864906}, {'text': 'SQL', 'type': 'MISC', 'score': 0.8678936}, {'text': 'Amazon', 'type': 'ORG', 'score': 0.9870702}, {'text': 'RDS', 'type': 'ORG', 'score': 0.95505756}, {'text': 'SQL', 'type': 'MISC', 'score': 0.8227789}, {'text': 'Amazon', 'type': 'ORG', 'score': 0.9982017}, {'text': 'SQL', 'type': 'MISC', 'score': 0.7555069}, {'text': 'Aurora', 'type': 'ORG', 'score': 0.87105393}, {'text': 'MySQL', 'type': 'MISC', 'score': 0.9224081}, {'text': 'Aurora', 'type': 'ORG', 'score': 0.9806912}, {'text': 'Neptune', 'type': 'ORG', 'score': 0.67683995}, {'text': 'Quantum Ledger', 'type': 'MISC', 'score': 0.9573109}, {'text': 'Oracle', 'type': 'ORG', 'score': 0.8935214}]}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# AGENT:","metadata":{"id":"aZtM2KoBuAcK"}},{"cell_type":"code","source":"llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash-lite-preview\", temperature=0.2)","metadata":{"id":"j051V81WP0qV","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:54:48.395445Z","iopub.execute_input":"2025-05-22T14:54:48.396083Z","iopub.status.idle":"2025-05-22T14:54:48.402757Z","shell.execute_reply.started":"2025-05-22T14:54:48.396035Z","shell.execute_reply":"2025-05-22T14:54:48.401979Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"search = DuckDuckGoSearchRun()\nDDSearch = Tool(name=\"Search\", func=search.run, description=\"Useful for looking up facts or credible URLs\"  )","metadata":{"id":"0ebBZB8YP2sH","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:54:49.836648Z","iopub.execute_input":"2025-05-22T14:54:49.836968Z","iopub.status.idle":"2025-05-22T14:54:49.841577Z","shell.execute_reply.started":"2025-05-22T14:54:49.836947Z","shell.execute_reply":"2025-05-22T14:54:49.840715Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"tools = [\n    DDSearch\n]","metadata":{"id":"fW08uyonP6f1","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:26:08.160080Z","iopub.execute_input":"2025-05-22T15:26:08.160409Z","iopub.status.idle":"2025-05-22T15:26:08.165016Z","shell.execute_reply.started":"2025-05-22T15:26:08.160357Z","shell.execute_reply":"2025-05-22T15:26:08.164143Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"agent = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=False,\n    handle_parsing_errors=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:26:15.590408Z","iopub.execute_input":"2025-05-22T15:26:15.590982Z","iopub.status.idle":"2025-05-22T15:26:15.595270Z","shell.execute_reply.started":"2025-05-22T15:26:15.590956Z","shell.execute_reply":"2025-05-22T15:26:15.594558Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"def fact_check_claim(claim, context):\n\n    prompt = f\"\"\"\n    You are a fact-checker with access to this tool:\n    - duckduck: Useful for finding credible URLs.\n    \n    \n    Given the following context:\n    \"{context}\"\n    \n    Fact-check this claim: \"{claim}\"\n    \n    Important rules:\n    1. Ignore spelling mistakes\n    2. If uncertain, return \"Unverifiable\"\n    3. Get me three credible URLs. \n    4. Use the search tool if you can't get URLs on your own.\n    \n    Final Answer: <respond in JSON like below>\n    \n    {{\n      \"verdict\": \"<Correct | Incorrect | Misleading | Unverifiable>\",\n      \"explanation\": \"<explanation>\",\"sources\": [\"<url1>\", \"<url2>\", \"<url3>\"],\"confidence\": \"<percentage>\"}}\n    \n    \"\"\"\n    \n    result = agent.invoke(prompt)\n    return result\n\nresult = fact_check_claim(\"Paris is in Italy.\", \"France is in Europe\")\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:26:16.459983Z","iopub.execute_input":"2025-05-22T15:26:16.460742Z","iopub.status.idle":"2025-05-22T15:26:22.744210Z","shell.execute_reply.started":"2025-05-22T15:26:16.460715Z","shell.execute_reply":"2025-05-22T15:26:22.743336Z"}},"outputs":[{"name":"stdout","text":"{'input': '\\n    You are a fact-checker with access to this tool:\\n    - duckduck: Useful for finding credible URLs.\\n    \\n    \\n    Given the following context:\\n    \"France is in Europe\"\\n    \\n    Fact-check this claim: \"Paris is in Italy.\"\\n    \\n    Important rules:\\n    1. Ignore spelling mistakes\\n    2. If uncertain, return \"Unverifiable\"\\n    3. Get me three credible URLs. \\n    4. Use the search tool if you can\\'t get URLs on your own.\\n    \\n    Final Answer: <respond in JSON like below>\\n    \\n    {\\n      \"verdict\": \"<Correct | Incorrect | Misleading | Unverifiable>\",\\n      \"explanation\": \"<explanation>\",\"sources\": [\"<url1>\", \"<url2>\", \"<url3>\"],\"confidence\": \"<percentage>\"}\\n    \\n    ', 'output': '```json\\n{\\n  \"verdict\": \"Incorrect\",\\n  \"explanation\": \"Paris is the capital of France, not Italy.\",\\n  \"sources\": [\\n    \"https://www.britannica.com/place/Paris\",\\n    \"https://www.worldatlas.com/cities/where-is-paris-located.html\",\\n    \"https://www.britannica.com/place/France\"\\n  ],\\n  \"confidence\": \"100%\"\\n}\\n```'}\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"print(result['output'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:26:25.162129Z","iopub.execute_input":"2025-05-22T15:26:25.162465Z","iopub.status.idle":"2025-05-22T15:26:25.166968Z","shell.execute_reply.started":"2025-05-22T15:26:25.162441Z","shell.execute_reply":"2025-05-22T15:26:25.166182Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"verdict\": \"Incorrect\",\n  \"explanation\": \"Paris is the capital of France, not Italy.\",\n  \"sources\": [\n    \"https://www.britannica.com/place/Paris\",\n    \"https://www.worldatlas.com/cities/where-is-paris-located.html\",\n    \"https://www.britannica.com/place/France\"\n  ],\n  \"confidence\": \"100%\"\n}\n```\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"import re\nimport json\n\ndef extract_json_from_output(result):\n    output_text = result['output']\n\n    # Try to extract JSON block (inside ```json ... ``` or just curly braces)\n    match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', output_text, re.DOTALL)\n\n    if not match:\n        # Fallback: try to find any JSON-looking block without the backticks\n        match = re.search(r'(\\{.*?\\})', output_text, re.DOTALL)\n\n    if match:\n        json_text = match.group(1)\n        try:\n            data = json.loads(json_text)\n            return data\n        except json.JSONDecodeError as e:\n            return {\"error\": f\"Failed to parse JSON: {e}\"}\n    else:\n        return {\"error\": \"No JSON found in output.\"}\n\n# Example usage\nresult = {\n  'input': '...',\n  'output': '```json\\n{\\n  \"verdict\": \"Incorrect\",\\n  \"explanation\": \"Paris is the capital of France, not Italy.\",\\n  \"sources\": [\\n    \"https://www.britannica.com/place/Paris\",\\n    \"https://www.worldatlas.com/cities/where-is-paris-located.html\",\\n    \"https://www.tripsavvy.com/is-paris-in-france-or-italy-7506704\"\\n  ],\\n  \"confidence\": \"100%\"\\n}\\n```'\n}\n\nparsed_output = extract_json_from_output(result)\nprint(json.dumps(parsed_output, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:26:27.568175Z","iopub.execute_input":"2025-05-22T15:26:27.568763Z","iopub.status.idle":"2025-05-22T15:26:27.575178Z","shell.execute_reply.started":"2025-05-22T15:26:27.568736Z","shell.execute_reply":"2025-05-22T15:26:27.574529Z"}},"outputs":[{"name":"stdout","text":"{\n  \"verdict\": \"Incorrect\",\n  \"explanation\": \"Paris is the capital of France, not Italy.\",\n  \"sources\": [\n    \"https://www.britannica.com/place/Paris\",\n    \"https://www.worldatlas.com/cities/where-is-paris-located.html\",\n    \"https://www.tripsavvy.com/is-paris-in-france-or-italy-7506704\"\n  ],\n  \"confidence\": \"100%\"\n}\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"video_url = \"https://www.youtube.com/watch?v=9GvgXI5BOws\"\ntranscript = get_youtube_transcript(video_url)\nfacts = extract_factual_claims(transcript)\n\nfor fact in facts:\n    print(f\"📝 Fact: {fact['sentence']}\")\n    result = fact_check_claim(fact['sentence'], fact['context'])\n    parsed_output = extract_json_from_output(result)\n    print(json.dumps(parsed_output, indent=2))\n    print(\"-\" * 50)\n    time.sleep(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:10:19.253493Z","iopub.execute_input":"2025-05-22T16:10:19.253786Z","iopub.status.idle":"2025-05-22T16:10:42.032392Z","shell.execute_reply.started":"2025-05-22T16:10:19.253765Z","shell.execute_reply":"2025-05-22T16:10:42.030986Z"}},"outputs":[{"name":"stdout","text":"📝 Fact: Was God a human invention?\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mDuckDuckGoSearchException\u001b[0m                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2308440556.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfacts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"📝 Fact: {fact['sentence']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfact_check_claim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfact\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mparsed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json_from_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3337782309.py\u001b[0m in \u001b[0;36mfact_check_claim\u001b[0;34m(claim, context)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             outputs = (\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \u001b[0;31m# We now enter the agent loop (until it returns something).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             next_step_output = self._take_next_step(\n\u001b[0m\u001b[1;32m   1621\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     ) -> Union[AgentFinish, list[tuple[AgentAction, str]]]:\n\u001b[1;32m   1325\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1326\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                 for a in self._iter_next_step(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     ) -> Union[AgentFinish, list[tuple[AgentAction, str]]]:\n\u001b[1;32m   1325\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1326\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                 for a in self._iter_next_step(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0magent_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent_action\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m             yield self._perform_agent_action(\n\u001b[0m\u001b[1;32m   1412\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_perform_agent_action\u001b[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 \u001b[0mtool_run_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"llm_prefix\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0;31m# We then call the tool on the tool input to get an observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             observation = tool.run(\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0magent_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mconfig_param\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0m_get_runnable_config_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mconfig_param\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtool_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"content_and_artifact\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/simple.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig_param\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0m_get_runnable_config_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_param\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tool does not support sync invocation.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mconfig_param\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0m_get_runnable_config_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mconfig_param\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtool_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"content_and_artifact\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/tools/ddg_search/tool.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m     72\u001b[0m     ) -> str:\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\"Use the tool.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;34m\"\"\"Run query through DuckDuckGo and return concatenated results.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ddgs_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"news\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ddgs_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py\u001b[0m in \u001b[0;36m_ddgs_text\u001b[0;34m(self, query, max_results)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDDGS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mddgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             ddgs_gen = ddgs.text(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mregion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/duckduckgo_search/duckduckgo_search.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, keywords, region, safesearch, timelimit, backend, max_results)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDuckDuckGoSearchException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     def _text_html(\n","\u001b[0;31mDuckDuckGoSearchException\u001b[0m: https://lite.duckduckgo.com/lite/ 202 Ratelimit"],"ename":"DuckDuckGoSearchException","evalue":"https://lite.duckduckgo.com/lite/ 202 Ratelimit","output_type":"error"}],"execution_count":122},{"cell_type":"markdown","source":"# LLM Chain:","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"from langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.tools import Tool\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain_community.tools import DuckDuckGoSearchRun\nimport re, json\n\n# Initialize LLM\nllm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash-lite-preview\", temperature=0.2)\n\n# Setup tools\nsearch = DuckDuckGoSearchRun()\ntools = {\n    \"duckduck\": search.run\n}\n\n# Define prompt template\nprompt_template = PromptTemplate(\n    input_variables=[\"context\", \"claim\"],\n    template=\"\"\"\nYou are a fact-checker with access to this tool:\n- duckduck: Useful for finding credible URLs.\n\n\nGiven the following context:\n\"{context}\"\n\nFact-check this claim: \"{claim}\"\n\nImportant rules:\n1. Ignore spelling mistakes\n2. If uncertain, return \"Unverifiable\"\n3. Use the search tool if you can't get URLs on your own\nFinal Answer: <respond in JSON like below>\n\n{{\n  \"verdict\": \"<Correct | Incorrect | Misleading | Unverifiable>\",\n  \"explanation\": \"<explanation>\",\n  \"sources\": [\"<url1>\", \"<url2>\", \"<url3>\"],\n  \"confidence\": \"<percentage>\"\n}}\n\n\"\"\"\n)\n\n# Create LLMChain\nllm_chain = LLMChain(llm=llm,prompt=prompt_template)\n","metadata":{"id":"9XaImbao8M1u","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:22:40.731723Z","iopub.execute_input":"2025-05-22T15:22:40.732459Z","iopub.status.idle":"2025-05-22T15:22:40.740850Z","shell.execute_reply.started":"2025-05-22T15:22:40.732432Z","shell.execute_reply":"2025-05-22T15:22:40.740136Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"def fact_check_claim(claim, context):\n    result = llm_chain.run(context=context, claim=claim)\n\n    # Get actual text content\n    output_text = result.content if hasattr(result, 'content') else str(result)\n\n    # Strip code block markers if present\n    output_text = output_text.strip(\"```json\").strip(\"```\").strip()\n\n    try:\n        json_result = json.loads(output_text)\n        return json_result\n    except Exception as e:\n        print(\"Failed to parse JSON:\", e)\n        print(\"Raw LLM Output:\", output_text)\n        return {\n            \"verdict\": \"Unclear\",\n            \"explanation\": \"Failed to parse result.\",\n            \"sources\": []\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:54:33.371439Z","iopub.execute_input":"2025-05-22T14:54:33.372103Z","iopub.status.idle":"2025-05-22T14:54:33.377650Z","shell.execute_reply.started":"2025-05-22T14:54:33.372075Z","shell.execute_reply":"2025-05-22T14:54:33.376744Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"print(fact_check_claim(context=\"The claim relates to current global temperature trends.\", claim=\"Global temperatures have dropped since 2020.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:54:34.539863Z","iopub.execute_input":"2025-05-22T14:54:34.540167Z","iopub.status.idle":"2025-05-22T14:54:35.946682Z","shell.execute_reply.started":"2025-05-22T14:54:34.540145Z","shell.execute_reply":"2025-05-22T14:54:35.945923Z"}},"outputs":[{"name":"stdout","text":"{'verdict': 'Incorrect', 'explanation': 'Multiple sources indicate that global temperatures have continued to increase since 2020, not decreased. 2023 was the warmest year on record, and the trend continues upward.', 'sources': ['https://climate.nasa.gov/vital-signs/global-temperature/', 'https://www.ncei.noaa.gov/global/temperature', 'https://www.carbonbrief.org/state-of-the-climate-global-emissions-reach-record-high-in-2023/'], 'confidence': '99'}\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"# Google Tools","metadata":{}},{"cell_type":"code","source":"from google.generativeai import GenerativeModel\nimport google.generativeai # Import the main library\n\n# List available models\n# models = GenerativeModel.list() # Incorrect way to list models\nmodels = google.generativeai.list_models() # Correct way to list models\nfor m in models:\n    print(m.name, m.description)","metadata":{"id":"ozUD6MoLnBhL","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:29:25.601837Z","iopub.execute_input":"2025-05-22T14:29:25.602149Z","iopub.status.idle":"2025-05-22T14:29:25.711673Z","shell.execute_reply.started":"2025-05-22T14:29:25.602124Z","shell.execute_reply":"2025-05-22T14:29:25.710745Z"}},"outputs":[{"name":"stdout","text":"models/embedding-gecko-001 Obtain a distributed representation of a text.\nmodels/gemini-1.0-pro-vision-latest The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\nmodels/gemini-pro-vision The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\nmodels/gemini-1.5-pro-latest Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\nmodels/gemini-1.5-pro-001 Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\nmodels/gemini-1.5-pro-002 Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\nmodels/gemini-1.5-pro Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\nmodels/gemini-1.5-flash-latest Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\nmodels/gemini-1.5-flash-001 Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\nmodels/gemini-1.5-flash-001-tuning Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\nmodels/gemini-1.5-flash Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\nmodels/gemini-1.5-flash-002 Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\nmodels/gemini-1.5-flash-8b Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-001 Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-latest Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-exp-0827 Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\nmodels/gemini-1.5-flash-8b-exp-0924 Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\nmodels/gemini-2.5-pro-exp-03-25 Experimental release (March 25th, 2025) of Gemini 2.5 Pro\nmodels/gemini-2.5-pro-preview-03-25 Gemini 2.5 Pro Preview 03-25\nmodels/gemini-2.5-flash-preview-04-17 Preview release (April 17th, 2025) of Gemini 2.5 Flash\nmodels/gemini-2.5-flash-preview-05-20 Preview release (April 17th, 2025) of Gemini 2.5 Flash\nmodels/gemini-2.5-flash-preview-04-17-thinking Preview release (April 17th, 2025) of Gemini 2.5 Flash\nmodels/gemini-2.5-pro-preview-05-06 Preview release (May 6th, 2025) of Gemini 2.5 Pro\nmodels/gemini-2.0-flash-exp Gemini 2.0 Flash Experimental\nmodels/gemini-2.0-flash Gemini 2.0 Flash\nmodels/gemini-2.0-flash-001 Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\nmodels/gemini-2.0-flash-lite-001 Stable version of Gemini 2.0 Flash Lite\nmodels/gemini-2.0-flash-lite Gemini 2.0 Flash-Lite\nmodels/gemini-2.0-flash-lite-preview-02-05 Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\nmodels/gemini-2.0-flash-lite-preview Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\nmodels/gemini-2.0-pro-exp Experimental release (March 25th, 2025) of Gemini 2.5 Pro\nmodels/gemini-2.0-pro-exp-02-05 Experimental release (March 25th, 2025) of Gemini 2.5 Pro\nmodels/gemini-exp-1206 Experimental release (March 25th, 2025) of Gemini 2.5 Pro\nmodels/gemini-2.0-flash-thinking-exp-01-21 Preview release (April 17th, 2025) of Gemini 2.5 Flash\nmodels/gemini-2.0-flash-thinking-exp Preview release (April 17th, 2025) of Gemini 2.5 Flash\nmodels/gemini-2.0-flash-thinking-exp-1219 Preview release (April 17th, 2025) of Gemini 2.5 Flash\nmodels/gemini-2.5-flash-preview-tts Gemini 2.5 Flash Preview TTS\nmodels/gemini-2.5-pro-preview-tts Gemini 2.5 Pro Preview TTS\nmodels/learnlm-2.0-flash-experimental LearnLM 2.0 Flash Experimental\nmodels/gemma-3-1b-it \nmodels/gemma-3-4b-it \nmodels/gemma-3-12b-it \nmodels/gemma-3-27b-it \nmodels/gemma-3n-e4b-it \nmodels/embedding-001 Obtain a distributed representation of a text.\nmodels/text-embedding-004 Obtain a distributed representation of a text.\nmodels/gemini-embedding-exp-03-07 Obtain a distributed representation of a text.\nmodels/gemini-embedding-exp Obtain a distributed representation of a text.\nmodels/aqa Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\nmodels/imagen-3.0-generate-002 Vertex served Imagen 3.0 002 model\nmodels/gemini-2.5-flash-preview-native-audio-dialog Gemini 2.5 Flash Preview Native Audio Dialog\nmodels/gemini-2.5-flash-exp-native-audio-thinking-dialog Gemini 2.5 Flash Exp Native Audio Thinking Dialog\nmodels/gemini-2.0-flash-live-001 Gemini 2.0 Flash 001\n","output_type":"stream"}],"execution_count":40}]}