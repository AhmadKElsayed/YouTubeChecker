{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRp0De1UxsyFCGBnfiWpv2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadKElsayed/YouTubeChecker/blob/main/YoutubeChecker_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQKtH7PcPTqk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "[YouTube URL] → [Extract Transcript]\n",
        "                      ↓\n",
        "                  [NER Pipeline]\n",
        "                      ↓\n",
        "               [Extract Facts / Claims]\n",
        "                      ↓\n",
        "             [Fact-check with Gemini]\n",
        "                      ↓\n",
        "         [Optionally: Google Search / SerpAPI]\n",
        "                      ↓\n",
        "       [Return verdict + explanation + URL]\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n",
        "!pip install youtube-transcript-api\n",
        "!pip install langchain\n",
        "!pip install langchain-google-genai\n",
        "!pip install transformers\n",
        "# !pip install serpapi\n",
        "!pip install dateparser\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install -U langchain-community\n",
        "!pip install duckduckgo-search\n",
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "Tp9Edjs8PiMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dateparser\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from transformers import pipeline\n",
        "from langchain.tools import Tool\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import spacy\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain.tools import Tool"
      ],
      "metadata": {
        "id": "0mzlqOTDPm88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "video_url = \"https://www.youtube.com/watch?v=JIbIYCM48to&t=99s\""
      ],
      "metadata": {
        "id": "RZ9fd5anPqnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_youtube_transcript(video_url):\n",
        "\n",
        "    video_id = video_url.split(\"v=\")[-1]\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "    text = \" \".join([item['text'] for item in transcript])\n",
        "\n",
        "    return text\n",
        "text = get_youtube_transcript(video_url)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "0uxcHOnGPtlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
        "\n",
        "# Load spaCy and Hugging Face NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from transformers import pipeline\n",
        "import spacy\n",
        "import dateparser\n",
        "\n",
        "# Load models\n",
        "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_factual_claims(text):\n",
        "    # Process the transcript into sentences\n",
        "    doc = nlp(text)\n",
        "    extracted_claims = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sentence_text = sent.text.strip()\n",
        "        if len(sentence_text) < 5:\n",
        "            continue\n",
        "\n",
        "        # Run NER on each sentence individually\n",
        "        entities = ner_pipeline(sentence_text)\n",
        "\n",
        "        # Only keep sentences with detected entities\n",
        "        if entities:\n",
        "            claim_info = {\n",
        "                \"sentence\": sentence_text,\n",
        "                \"entities\": []\n",
        "            }\n",
        "            for ent in entities:\n",
        "                entity_data = {\n",
        "                    \"text\": ent['word'],\n",
        "                    \"type\": ent['entity_group'],\n",
        "                    \"score\": ent['score']\n",
        "                }\n",
        "                if ent['entity_group'] == \"DATE\":\n",
        "                    parsed_date = dateparser.parse(ent['word'])\n",
        "                    entity_data[\"parsed_date\"] = str(parsed_date) if parsed_date else None\n",
        "                claim_info[\"entities\"].append(entity_data)\n",
        "            extracted_claims.append(claim_info)\n",
        "\n",
        "    return extracted_claims\n",
        "\n",
        "\n",
        "# Example\n",
        "claims = extract_factual_claims(text)\n",
        "for c in claims[:5]:\n",
        "    print(c)\n"
      ],
      "metadata": {
        "id": "SVQ5lRzMPupi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGENT:"
      ],
      "metadata": {
        "id": "aZtM2KoBuAcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLM:"
      ],
      "metadata": {
        "id": "xe-cpWxduEZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash-lite-preview\", temperature=0.2)"
      ],
      "metadata": {
        "id": "j051V81WP0qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools:"
      ],
      "metadata": {
        "id": "ADBHlHoHuHeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Youtube_Transcriptor = Tool(name=\"YouTube Transcript\",func=get_youtube_transcript,description=\"Fetch transcript from a YouTube video given its video URL\")"
      ],
      "metadata": {
        "id": "nngwNu26uPi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = DuckDuckGoSearchRun()\n",
        "DDSearch = Tool(name=\"Search\", func=search.run, description=\"Useful for looking up facts or credible URLs\"  )"
      ],
      "metadata": {
        "id": "0ebBZB8YP2sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fact_claim_extractor = Tool(\n",
        "    name=\"FactClaimExtractor\",\n",
        "    func=lambda text: str(extract_factual_claims(text)),  # return as stringified JSON for LLM\n",
        "    description=\"Extracts factual claims and named entities from a given text transcript. Expects a plain text transcript.\"\n",
        ")"
      ],
      "metadata": {
        "id": "R1TAKmM9tUDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    fact_claim_extractor,\n",
        "    DDSearch\n",
        "]"
      ],
      "metadata": {
        "id": "fW08uyonP6f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "IVFhHA2PP7-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fact_check_claim(transcript):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Fact-check this transcript: \"{transcript}\"\n",
        "    First, extract any factual claims or named entities using FactClaimExtractor.\n",
        "    Then, fact-check those claims one by one.\n",
        "    - Respond with 'Correct', 'Incorrect', or 'Unverifiable'.\n",
        "    - Explain briefly why.\n",
        "    - Get me three credible URL on your own.\n",
        "    - If you couldn't find them on your own, call the Search tool to look up credible sources and then respond.\n",
        "    \"\"\"\n",
        "    result = agent.run(prompt)\n",
        "    return result\n",
        "\n",
        "result = fact_check_claim(\"Paris is in Italy. Napoleon was born there\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "A14otehXP-LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "video_url = \"https://www.youtube.com/watch?v=JIbIYCM48to&t=99s\"\n",
        "transcript = get_youtube_transcript(video_url)\n",
        "facts = extract_factual_claims(transcript)\n",
        "\n",
        "for fact in facts[:5]:\n",
        "    print(f\"📝 Fact: {fact}\")\n",
        "    result = fact_check_claim(fact)\n",
        "    print(result)\n",
        "    print(\"-\" * 50)\n",
        "    time.sleep(5)  # wait 5 seconds between calls"
      ],
      "metadata": {
        "id": "Xuy57VqHQEzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.generativeai import GenerativeModel\n",
        "import google.generativeai # Import the main library\n",
        "\n",
        "# List available models\n",
        "# models = GenerativeModel.list() # Incorrect way to list models\n",
        "models = google.generativeai.list_models() # Correct way to list models\n",
        "for m in models:\n",
        "    print(m.name, m.description)"
      ],
      "metadata": {
        "id": "ozUD6MoLnBhL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}